{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c244d7c6-45d3-4a3a-8d34-78e75fa8c843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matthias/workspace/Coding/ML_gh_repo_personal/MachineLearning_Python/yolo_v7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6937769b-bf42-4edc-8440-9c34b2176f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('workspace/Coding/ML_gh_repo_personal/MachineLearning_Python/yolo_v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2527896f-1b07-4ca4-be65-16df2ed63286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3599fd3a-67cf-4c6e-991a-1f2ab0f77022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import letterbox\n",
    "from models.experimental import attempt_load \n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint\n",
    "from utils.pose import show_image, plot_pose_prediction, make_pose_prediction, add_pose_in_video_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d33684-f2ba-4827-92e0-8f6b7b7aaaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# set gpu device if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4443414d-fb18-4bf3-98c9-2af03a2cdd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file:  weights_pretrained_models/yolov7_w6_pretrained_pose.pt\n",
      "File exists?  True\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/anaconda3/envs/cv_pose/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /workspace/croot/pytorch_1692381528408/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "#model = attempt_load('MachineLearning_Python/yolo_v7/weights_pretrained_models/yolov7_w6_pretrained_pose.pt', map_location=device)\n",
    "model = attempt_load('weights_pretrained_models/yolov7_w6_pretrained_pose.pt', map_location=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c32c459-1c56-49d6-ba16-f66211a067c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 1\n",
      "Number of keypoints: 17\n"
     ]
    }
   ],
   "source": [
    "# Switch to evaluation mode, map_location=device\n",
    "model.eval()\n",
    "\n",
    "print('Number of classes:', model.yaml['nc'])\n",
    "print('Number of keypoints:', model.yaml['nkpt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b780eabc-9cea-4f42-96ed-b8d3e40ff820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image (536, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "# read original image\n",
    "orig_img = cv2.imread('inference/images/image1.jpg')\n",
    "orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "print('Original image', orig_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd45e913-5d80-4cbc-90e1-44416e2350f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(6,6))\n",
    "plt.imshow(orig_img)\n",
    "plt.axis('off')\n",
    "plt.savefig('fig.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f27856f3-afa5-47d6-90a6-99417c11e041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized image (576, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "# resize and pad\n",
    "img = letterbox(orig_img, 640, stride=64, auto=True)[0]\n",
    "print('Resized image', img.shape)\n",
    "\n",
    "#plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.savefig('fig_resized.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eda22c-d924-44b4-a1c4-1f7d12852921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anchors:', model.yaml['anchors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311090a0-dcbb-4801-a560-c970fb87c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to tensor\n",
    "image_tensor = transforms.ToTensor()(img)\n",
    "# add dimension\n",
    "image_tensor = torch.unsqueeze(image_tensor, 0)\n",
    "print('Transformed to tensor image:', image_tensor.shape)\n",
    "# send the picture to the calculating device\n",
    "image_tensor = image_tensor.to(device).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred, _ = model(image_tensor)\n",
    "print('Predictions shape:', pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eba5b2-c4a1-47e7-8473-d1ff2949c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = non_max_suppression_kpt(pred, \n",
    "                               conf_thres=0.25, \n",
    "                               iou_thres=0.65, \n",
    "                               nc=model.yaml['nc'], \n",
    "                               nkpt=model.yaml['nkpt'], \n",
    "                               kpt_label=True)\n",
    "print('Detected poses:', len(pred))\n",
    "print('Prediction shape:', pred[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec93d6c-8526-4748-977d-2ab79a4ee597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = output_to_keypoint(pred)\n",
    "plot_pose_prediction(img, pred)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e072c39-fe7a-443c-bcf9-48e46e905f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(cv2.imread('breakdance.jpg'), cv2.COLOR_BGR2RGB)\n",
    "pred = make_pose_prediction(model, img)\n",
    "plot_pose_prediction(img, pred, show_bbox=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b426f46-cc50-419e-aba8-c0ecd8eef29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "add_pose_in_video_file('board.mp4', 'board_out.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
